{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\miche\\OneDrive\\Desktop\\Codenames Progetto PDE\\Codenames-LLM\\codenames_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import codenamesLLM\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 5/90 [00:00<00:01, 44.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped game between gpt-4o and gpt-4o-mini: billing problem\n",
      "skipped game between gpt-4o and gpt-3.5-turbo: billing problem\n",
      "skipped game between gpt-4o and grok-beta: billing problem\n",
      "skipped game between gpt-4o and claude-3-5-sonnet-latest: billing problem\n",
      "skipped game between gpt-4o and claude-3-5-haiku-latest: billing problem\n",
      "skipped game between gpt-4o and llama3.2-11b-vision: billing problem\n",
      "skipped game between gpt-4o and llama3.2-90b-vision: billing problem\n",
      "skipped game between gpt-4o-mini and gpt-4o: billing problem\n",
      "skipped game between gpt-4o-mini and gpt-3.5-turbo: billing problem\n",
      "skipped game between gpt-4o-mini and grok-beta: billing problem\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 16/90 [00:00<00:01, 49.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped game between gpt-4o-mini and claude-3-5-sonnet-latest: billing problem\n",
      "skipped game between gpt-4o-mini and claude-3-5-haiku-latest: billing problem\n",
      "skipped game between gpt-4o-mini and llama3.2-11b-vision: billing problem\n",
      "skipped game between gpt-4o-mini and llama3.2-90b-vision: billing problem\n",
      "skipped game between gpt-3.5-turbo and gpt-4o: billing problem\n",
      "skipped game between gpt-3.5-turbo and gpt-4o-mini: billing problem\n",
      "skipped game between gpt-3.5-turbo and grok-beta: billing problem\n",
      "Executed gpt-3.5-turbo vs grok-beta\n",
      "skipped game between gpt-3.5-turbo and claude-3-5-sonnet-latest: billing problem\n",
      "Executed gpt-3.5-turbo vs claude-3-5-sonnet-latest\n",
      "skipped game between gpt-3.5-turbo and claude-3-5-haiku-latest: billing problem\n",
      "Executed gpt-3.5-turbo vs claude-3-5-haiku-latest\n",
      "skipped game between gpt-3.5-turbo and llama3.2-11b-vision: billing problem\n",
      "Executed gpt-3.5-turbo vs llama3.2-11b-vision\n",
      "skipped game between gpt-3.5-turbo and llama3.2-90b-vision: billing problem\n",
      "Executed gpt-3.5-turbo vs llama3.2-90b-vision\n",
      "skipped game between grok-beta and gpt-4o: billing problem\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 22/90 [00:00<00:01, 50.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed grok-beta vs gpt-4o\n",
      "skipped game between grok-beta and gpt-4o-mini: billing problem\n",
      "Executed grok-beta vs gpt-4o-mini\n",
      "skipped game between grok-beta and gpt-3.5-turbo: billing problem\n",
      "Executed grok-beta vs gpt-3.5-turbo\n",
      "Playing grok-beta vs claude-3-5-sonnet-latest...\n",
      "Executed grok-beta vs claude-3-5-sonnet-latest\n",
      "Playing grok-beta vs claude-3-5-haiku-latest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 22/90 [00:14<00:01, 50.34it/s]"
     ]
    }
   ],
   "source": [
    "# Path to your Excel file\n",
    "file_path = \"experiment_data\\model_tournament\\model_tournament_mich1.xlsx\" #compy the model_tournament_input to use one new\n",
    "\n",
    "# Read the Excel file into a DataFrame\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Iterate through the rows using titled columns\n",
    "for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    # Access values using column titles\n",
    "    red_team = row['red_model']  # Replace with the actual column title for the red team\n",
    "    blue_team = row['blue_model']  # Replace with the actual column title for the blue team\n",
    "    not_usable = [\"gemini\", \"gpt\"] #need to reconfigure billing\n",
    "    playable = True\n",
    "\n",
    "    for model in not_usable:\n",
    "        if red_team.startswith(model) or blue_team.startswith(model):\n",
    "            print(f\"skipped game between {red_team} and {blue_team}: billing problem\")\n",
    "            playable = False\n",
    "\n",
    "    already_played = pd.notna(row['winner'])\n",
    "    \n",
    "    if ((pd.notna(red_team) and pd.notna(blue_team)) and not(already_played)) and playable:  # Check if both values are not NaN\n",
    "        try:\n",
    "            print(f\"Playing {red_team} vs {blue_team}...\")\n",
    "            # Call your function and get the result\n",
    "            result = codenamesLLM.play_game(red_model = red_team, blue_model = blue_team)\n",
    "\n",
    "            red_stats = codenamesLLM.analyze_team_guesses(result[3], \"RED\")\n",
    "            blue_stats = codenamesLLM.analyze_team_guesses(result[3], \"BLUE\")\n",
    "\n",
    "            df.at[index, 'red_model'] = red_team\n",
    "            df.at[index, 'red_model'] = red_team\n",
    "            df.at[index, 'winner'] = result[0]  \n",
    "            df.at[index, 'red_avg_words_2guess'] = red_stats['average_expected_guesses']\n",
    "            df.at[index, 'blue_avg_words_2guess'] = blue_stats['average_expected_guesses']\n",
    "            df.at[index, 'red_avg_words_guessed'] = red_stats['average_correct_guesses']\n",
    "            df.at[index, 'blue_avg_words_guessed'] = blue_stats['average_correct_guesses']\n",
    "            df.at[index, 'reason'] = result[1]\n",
    "            df.at[index, 'red_turns'] = red_stats['total_hints']\n",
    "            df.at[index, 'blue_turns'] = blue_stats['total_hints']\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"skipped game between {red_team} and {blue_team}: {e}\")\n",
    "\n",
    "    # Write the updated DataFrame back to the same Excel file\n",
    "    df.to_excel(file_path, index=False)\n",
    "    if not already_played:\n",
    "        print(f\"Executed {red_team} vs {blue_team}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codenames_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
